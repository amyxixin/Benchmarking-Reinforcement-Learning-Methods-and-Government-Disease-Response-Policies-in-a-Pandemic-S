{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import pandemic_simulator as ps\n",
    "import pickle\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.log_prob = []\n",
    "        self.rewards = []\n",
    "        self.terminals = []\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.log_p[:]\n",
    "        del self.rewards[:]\n",
    "        del self.terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "\n",
    "        # actor\n",
    "        self.actor = nn.Sequential(nn.Linear(num_states, 64), \n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 32),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(32, num_actions),\n",
    "                        nn.Softmax(dim=-1)\n",
    "                        )\n",
    "        \n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(num_states, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 32),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(32, 1)\n",
    "                    ) \n",
    "\n",
    "    def act(self, state, buffer):\n",
    "\n",
    "        action_prob = self.actor(state)\n",
    "        dist = Categorical(action_prob)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        buffer.states.append(state)\n",
    "        buffer.actions.append(action)\n",
    "        buffer.log_probs.append(action_logprob)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        action_prob = self.actor(state)\n",
    "        dist = Categorial(action_prob)\n",
    "        dist_entropy = dist.entropy()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_value = self.critic(state)\n",
    "        \n",
    "        return action_logprob, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, num_states, num_actions, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.K_epochs = K_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        \n",
    "        # current policy\n",
    "        self.policy = ActorCritic(num_states, num_actions).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        \n",
    "        # old policy\n",
    "        self.policy_old = ActorCritic(num_states, num_actions).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        # loss function\n",
    "        self.loss_fcn = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state, buffer):\n",
    "\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\n",
    "        return self.policy_old.act(state, buffer)\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.terminals)):\n",
    "            if terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.log_p, dim=0)).detach().to(device)\n",
    "\n",
    "        # Train policy\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # importance ratio p/q\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            advantages = rewards - state_values.detach()\n",
    "            # actor loss using surrogate loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            actor_loss = - torch.min(surr1, surr2)\n",
    "            # critic loss\n",
    "            critic_loss = 0.5 * self.loss_fcn(rewards, state_values) - 0.01 * dist_entropy\n",
    "\n",
    "            # total loss of clipped objective PPO\n",
    "            loss = actor_loss + critic_loss\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, num_states, num_actions, max_episodes, max_timesteps, update_timestep, K_epochs, eps_clip, \n",
    "          gamma, lr, betas, print_interval=10,):\n",
    "\n",
    "    buffer = RolloutBuffer()\n",
    "\n",
    "    ppo = PPO(num_states, num_actions, lr, betas, gamma, K_epochs, eps_clip)\n",
    "\n",
    "    running_reward, avg_length, time_step = 0, 0, 0\n",
    "\n",
    "    # training loop\n",
    "    for episode in range(1, max_episodes+1):\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            time_step += 1\n",
    "\n",
    "            # Run old policy\n",
    "            action = ppo.select_action(state, memory)\n",
    "\n",
    "            state, reward, terminal, _ = env.step(action)\n",
    "\n",
    "            buffer.rewards.append(reward)\n",
    "            buffer.terminals.append(done)\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                ppo.update(buffer)\n",
    "                buffer.clear()\n",
    "                time_step = 0\n",
    "\n",
    "            running_reward += reward\n",
    "\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        avg_length += t\n",
    "        \n",
    "        viz.record((state, reward))\n",
    "        if episode % print_interval == 0:\n",
    "            avg_length = int(avg_length / print_interval)\n",
    "            running_reward = int((running_reward / print_interval))\n",
    "\n",
    "            print('Episode {} \\t Avg length: {} \\t Avg reward: {}'.format(episode, avg_length, running_reward))\n",
    "\n",
    "            running_reward, avg_length = 0, 0\n",
    "        if episode < max_episodes+1:\n",
    "            viz = ps.viz.GymViz.from_config(sim_config=sim_config)\n",
    "            \n",
    "\n",
    "    viz.plot()\n",
    "    \n",
    "    with open('results.pickle', 'wb') as handle:\n",
    "        pickle.dump(viz, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(10)\n",
    "# np.random.seed(10)\n",
    "ps.init_globals(0)\n",
    "\n",
    "# select a simulator config\n",
    "sim_config = ps.sh.small_town_config\n",
    "# make environment\n",
    "env = ps.env.PandemicGymEnv.from_config(sim_config, pandemic_regulations=ps.sh.austin_regulations)\n",
    "\n",
    "# setup viz\n",
    "viz = ps.viz.GymViz.from_config(sim_config=sim_config)\n",
    "\n",
    "# num_states = env.observation_space.shape[0]\n",
    "# num_actions = env.action_space.n\n",
    "num_states = 12\n",
    "num_actions = 5\n",
    "\n",
    "# HYPER PARAMS\n",
    "max_episodes = 100000\n",
    "max_timesteps = 1500\n",
    "update_timestep = 4000\n",
    "K_epochs = 80\n",
    "eps_clip = 0.2\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "\n",
    "train(env, num_states, num_actions, max_episodes, max_timesteps, update_timestep, K_epochs, eps_clip, gamma, lr, betas=[0.9, 0.990], print_interval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
